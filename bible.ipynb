{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dagbo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import punkt\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import sklearn\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               citation        book  chapter  verse  \\\n",
      "0           Genesis 1:1     Genesis        1      1   \n",
      "1           Genesis 1:2     Genesis        1      2   \n",
      "2           Genesis 1:3     Genesis        1      3   \n",
      "3           Genesis 1:4     Genesis        1      4   \n",
      "4           Genesis 1:5     Genesis        1      5   \n",
      "...                 ...         ...      ...    ...   \n",
      "31097  Revelation 22:17  Revelation       22     17   \n",
      "31098  Revelation 22:18  Revelation       22     18   \n",
      "31099  Revelation 22:19  Revelation       22     19   \n",
      "31100  Revelation 22:20  Revelation       22     20   \n",
      "31101  Revelation 22:21  Revelation       22     21   \n",
      "\n",
      "                                                    text  \n",
      "0      In the beginning God created the heaven and th...  \n",
      "1      And the earth was without form, and void; and ...  \n",
      "2      And God said, Let there be light: and there wa...  \n",
      "3      And God saw the light, that it was good: and G...  \n",
      "4      And God called the light Day, and the darkness...  \n",
      "...                                                  ...  \n",
      "31097  And the Spirit and the bride say, Come. And le...  \n",
      "31098  For I testify unto every man that heareth the ...  \n",
      "31099  And if any man shall take away from the words ...  \n",
      "31100  He which testifieth these things saith, Surely...  \n",
      "31101  The grace of our Lord Jesus Christ be with you...  \n",
      "\n",
      "[31102 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "bible_df = pd.read_csv(r\"bible_data_set.csv\")\n",
    "print(bible_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "citation    object\nbook        object\nchapter      int64\nverse        int64\ntext        object\ndtype: object"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_df.head(5)\n",
    "bible_df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [citation, book, chapter, verse, text]\n",
      "Index: []\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31102 entries, 0 to 31101\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   citation  31102 non-null  object\n",
      " 1   book      31102 non-null  object\n",
      " 2   chapter   31102 non-null  int64 \n",
      " 3   verse     31102 non-null  int64 \n",
      " 4   text      31102 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.2+ MB\n",
      "(31102, 5)\n"
     ]
    }
   ],
   "source": [
    "#check if the dataset has a null data\n",
    "\n",
    "missing_data = bible_df[bible_df.isna().any(axis=1)]\n",
    "print(missing_data)\n",
    "bible_df.info()\n",
    "print(bible_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# drop rows with missing data\n",
    "bible_df.dropna(inplace=True)\n",
    "\n",
    "# check for and remove duplicates\n",
    "bible_df.drop_duplicates(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizing the text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# convert all text to lowercase\n",
    "bible_df[\"text\"] = bible_df[\"text\"].str.lower()\n",
    "\n",
    "# Remove special characters and numbers\n",
    "bible_df[\"text\"] = bible_df[\"text\"].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenize the text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# tokenize the text\n",
    "bible_df[\"text\"] = bible_df[\"text\"].apply(nltk.word_tokenize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "               citation        book  chapter  verse  \\\n0           Genesis 1:1     Genesis        1      1   \n1           Genesis 1:2     Genesis        1      2   \n2           Genesis 1:3     Genesis        1      3   \n3           Genesis 1:4     Genesis        1      4   \n4           Genesis 1:5     Genesis        1      5   \n...                 ...         ...      ...    ...   \n31097  Revelation 22:17  Revelation       22     17   \n31098  Revelation 22:18  Revelation       22     18   \n31099  Revelation 22:19  Revelation       22     19   \n31100  Revelation 22:20  Revelation       22     20   \n31101  Revelation 22:21  Revelation       22     21   \n\n                                                    text  \n0      [in, the, beginning, god, created, the, heaven...  \n1      [and, the, earth, was, without, form, and, voi...  \n2      [and, god, said, let, there, be, light, and, t...  \n3      [and, god, saw, the, light, that, it, was, goo...  \n4      [and, god, called, the, light, day, and, the, ...  \n...                                                  ...  \n31097  [and, the, spirit, and, the, bride, say, come,...  \n31098  [for, i, testify, unto, every, man, that, hear...  \n31099  [and, if, any, man, shall, take, away, from, t...  \n31100  [he, which, testifieth, these, things, saith, ...  \n31101  [the, grace, of, our, lord, jesus, christ, be,...  \n\n[31102 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>citation</th>\n      <th>book</th>\n      <th>chapter</th>\n      <th>verse</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Genesis 1:1</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[in, the, beginning, god, created, the, heaven...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Genesis 1:2</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[and, the, earth, was, without, form, and, voi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Genesis 1:3</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>3</td>\n      <td>[and, god, said, let, there, be, light, and, t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Genesis 1:4</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>4</td>\n      <td>[and, god, saw, the, light, that, it, was, goo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Genesis 1:5</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>5</td>\n      <td>[and, god, called, the, light, day, and, the, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31097</th>\n      <td>Revelation 22:17</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>17</td>\n      <td>[and, the, spirit, and, the, bride, say, come,...</td>\n    </tr>\n    <tr>\n      <th>31098</th>\n      <td>Revelation 22:18</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>18</td>\n      <td>[for, i, testify, unto, every, man, that, hear...</td>\n    </tr>\n    <tr>\n      <th>31099</th>\n      <td>Revelation 22:19</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>19</td>\n      <td>[and, if, any, man, shall, take, away, from, t...</td>\n    </tr>\n    <tr>\n      <th>31100</th>\n      <td>Revelation 22:20</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>20</td>\n      <td>[he, which, testifieth, these, things, saith, ...</td>\n    </tr>\n    <tr>\n      <th>31101</th>\n      <td>Revelation 22:21</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>21</td>\n      <td>[the, grace, of, our, lord, jesus, christ, be,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>31102 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stemming and lemmatizing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "bible_df[\"text\"] = bible_df[\"text\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "bible_df[\"text\"] = bible_df[\"text\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "               citation        book  chapter  verse  \\\n0           Genesis 1:1     Genesis        1      1   \n1           Genesis 1:2     Genesis        1      2   \n2           Genesis 1:3     Genesis        1      3   \n3           Genesis 1:4     Genesis        1      4   \n4           Genesis 1:5     Genesis        1      5   \n...                 ...         ...      ...    ...   \n31097  Revelation 22:17  Revelation       22     17   \n31098  Revelation 22:18  Revelation       22     18   \n31099  Revelation 22:19  Revelation       22     19   \n31100  Revelation 22:20  Revelation       22     20   \n31101  Revelation 22:21  Revelation       22     21   \n\n                                                    text  \n0      [in, the, begin, god, creat, the, heaven, and,...  \n1      [and, the, earth, wa, without, form, and, void...  \n2      [and, god, said, let, there, be, light, and, t...  \n3      [and, god, saw, the, light, that, it, wa, good...  \n4      [and, god, call, the, light, day, and, the, da...  \n...                                                  ...  \n31097  [and, the, spirit, and, the, bride, say, come,...  \n31098  [for, i, testifi, unto, everi, man, that, hear...  \n31099  [and, if, ani, man, shall, take, away, from, t...  \n31100  [he, which, testifieth, these, thing, saith, s...  \n31101  [the, grace, of, our, lord, jesu, christ, be, ...  \n\n[31102 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>citation</th>\n      <th>book</th>\n      <th>chapter</th>\n      <th>verse</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Genesis 1:1</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[in, the, begin, god, creat, the, heaven, and,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Genesis 1:2</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[and, the, earth, wa, without, form, and, void...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Genesis 1:3</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>3</td>\n      <td>[and, god, said, let, there, be, light, and, t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Genesis 1:4</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>4</td>\n      <td>[and, god, saw, the, light, that, it, wa, good...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Genesis 1:5</td>\n      <td>Genesis</td>\n      <td>1</td>\n      <td>5</td>\n      <td>[and, god, call, the, light, day, and, the, da...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31097</th>\n      <td>Revelation 22:17</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>17</td>\n      <td>[and, the, spirit, and, the, bride, say, come,...</td>\n    </tr>\n    <tr>\n      <th>31098</th>\n      <td>Revelation 22:18</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>18</td>\n      <td>[for, i, testifi, unto, everi, man, that, hear...</td>\n    </tr>\n    <tr>\n      <th>31099</th>\n      <td>Revelation 22:19</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>19</td>\n      <td>[and, if, ani, man, shall, take, away, from, t...</td>\n    </tr>\n    <tr>\n      <th>31100</th>\n      <td>Revelation 22:20</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>20</td>\n      <td>[he, which, testifieth, these, thing, saith, s...</td>\n    </tr>\n    <tr>\n      <th>31101</th>\n      <td>Revelation 22:21</td>\n      <td>Revelation</td>\n      <td>22</td>\n      <td>21</td>\n      <td>[the, grace, of, our, lord, jesu, christ, be, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>31102 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encoding categorical variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "\n",
    "# encode categorical variable 'book'\n",
    "le = LabelEncoder()\n",
    "bible_df[\"book\"] = le.fit_transform(bible_df[\"book\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 Chronicles' '1 Corinthians' '1 John' '1 Kings' '1 Peter' '1 Samuel'\n",
      " '1 Thessalonians' '1 Timothy' '2 Chronicles' '2 Corinthians' '2 John'\n",
      " '2 Kings' '2 Peter' '2 Samuel' '2 Thessalonians' '2 Timothy' '3 John'\n",
      " 'Acts' 'Amos' 'Colossians' 'Daniel' 'Deuteronomy' 'Ecclesiastes'\n",
      " 'Ephesians' 'Esther' 'Exodus' 'Ezekiel' 'Ezra' 'Galatians' 'Genesis'\n",
      " 'Habakkuk' 'Haggai' 'Hebrews' 'Hosea' 'Isaiah' 'James' 'Jeremiah' 'Job'\n",
      " 'Joel' 'John' 'Jonah' 'Joshua' 'Jude' 'Judges' 'Lamentations' 'Leviticus'\n",
      " 'Luke' 'Malachi' 'Mark' 'Matthew' 'Micah' 'Nahum' 'Nehemiah' 'Numbers'\n",
      " 'Obadiah' 'Philemon' 'Philippians' 'Proverbs' 'Psalms' 'Revelation'\n",
      " 'Romans' 'Ruth' 'Song of Solomon' 'Titus' 'Zechariah' 'Zephaniah']\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "               citation  book  chapter  verse  \\\n0           Genesis 1:1    29        1      1   \n1           Genesis 1:2    29        1      2   \n2           Genesis 1:3    29        1      3   \n3           Genesis 1:4    29        1      4   \n4           Genesis 1:5    29        1      5   \n...                 ...   ...      ...    ...   \n31097  Revelation 22:17    59       22     17   \n31098  Revelation 22:18    59       22     18   \n31099  Revelation 22:19    59       22     19   \n31100  Revelation 22:20    59       22     20   \n31101  Revelation 22:21    59       22     21   \n\n                                                    text  \n0      [in, the, begin, god, creat, the, heaven, and,...  \n1      [and, the, earth, wa, without, form, and, void...  \n2      [and, god, said, let, there, be, light, and, t...  \n3      [and, god, saw, the, light, that, it, wa, good...  \n4      [and, god, call, the, light, day, and, the, da...  \n...                                                  ...  \n31097  [and, the, spirit, and, the, bride, say, come,...  \n31098  [for, i, testifi, unto, everi, man, that, hear...  \n31099  [and, if, ani, man, shall, take, away, from, t...  \n31100  [he, which, testifieth, these, thing, saith, s...  \n31101  [the, grace, of, our, lord, jesu, christ, be, ...  \n\n[31102 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>citation</th>\n      <th>book</th>\n      <th>chapter</th>\n      <th>verse</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Genesis 1:1</td>\n      <td>29</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[in, the, begin, god, creat, the, heaven, and,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Genesis 1:2</td>\n      <td>29</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[and, the, earth, wa, without, form, and, void...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Genesis 1:3</td>\n      <td>29</td>\n      <td>1</td>\n      <td>3</td>\n      <td>[and, god, said, let, there, be, light, and, t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Genesis 1:4</td>\n      <td>29</td>\n      <td>1</td>\n      <td>4</td>\n      <td>[and, god, saw, the, light, that, it, wa, good...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Genesis 1:5</td>\n      <td>29</td>\n      <td>1</td>\n      <td>5</td>\n      <td>[and, god, call, the, light, day, and, the, da...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31097</th>\n      <td>Revelation 22:17</td>\n      <td>59</td>\n      <td>22</td>\n      <td>17</td>\n      <td>[and, the, spirit, and, the, bride, say, come,...</td>\n    </tr>\n    <tr>\n      <th>31098</th>\n      <td>Revelation 22:18</td>\n      <td>59</td>\n      <td>22</td>\n      <td>18</td>\n      <td>[for, i, testifi, unto, everi, man, that, hear...</td>\n    </tr>\n    <tr>\n      <th>31099</th>\n      <td>Revelation 22:19</td>\n      <td>59</td>\n      <td>22</td>\n      <td>19</td>\n      <td>[and, if, ani, man, shall, take, away, from, t...</td>\n    </tr>\n    <tr>\n      <th>31100</th>\n      <td>Revelation 22:20</td>\n      <td>59</td>\n      <td>22</td>\n      <td>20</td>\n      <td>[he, which, testifieth, these, thing, saith, s...</td>\n    </tr>\n    <tr>\n      <th>31101</th>\n      <td>Revelation 22:21</td>\n      <td>59</td>\n      <td>22</td>\n      <td>21</td>\n      <td>[the, grace, of, our, lord, jesu, christ, be, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>31102 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "train_df, val_df = train_test_split(bible_df, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained weights of the Bert model and create the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Convert the data to a list of dictionaries\n",
    "train_data = []\n",
    "for i in range(train_df.shape[0]):\n",
    "    input_ids = tokenizer.encode(train_df.iloc[i][\"text\"], return_tensors=\"pt\").squeeze()\n",
    "    attention_mask = (input_ids > 0).long()\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "    label = train_df.iloc[i][\"text\"]\n",
    "    train_data.append(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"token_type_ids\": token_type_ids, \"label\": label}\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "val_data = []\n",
    "for i in range(val_df.shape[0]):\n",
    "    input_ids = tokenizer.encode(val_df.iloc[i][\"text\"], return_tensors=\"pt\").squeeze()\n",
    "    attention_mask = (input_ids > 0).long()\n",
    "    token_type_ids = torch.zeros_like(input_ids)\n",
    "    label = val_df.iloc[i][\"book\"]\n",
    "    val_data.append(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"token_type_ids\": token_type_ids, \"label\": label}\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Create a DataLoader for the train and validation sets\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=32)\n",
    "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataCollatorForLanguageModeling' object has no attribute 'convert_to_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[63], line 6\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Convert the data to the correct format\u001B[39;00m\n\u001B[0;32m      3\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForLanguageModeling(\n\u001B[0;32m      4\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, mlm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, mlm_probability\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.15\u001B[39m\n\u001B[0;32m      5\u001B[0m )\n\u001B[1;32m----> 6\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_dataset\u001B[49m(train_df)\n\u001B[0;32m      7\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m data_collator\u001B[38;5;241m.\u001B[39mconvert_to_dataset(val_df)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataCollatorForLanguageModeling' object has no attribute 'convert_to_dataset'"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "# Convert the data to the correct format\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, mlm_probability=0.15\n",
    ")\n",
    "train_dataset = data_collator.convert_to_dataset(train_df)\n",
    "val_dataset = data_collator.convert_to_dataset(val_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dagbo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 24881\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2334\n",
      "  Number of trainable parameters = 109483778\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[62], line 25\u001B[0m\n\u001B[0;32m     17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     18\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     19\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m     20\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtrain_data,\n\u001B[0;32m     21\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mval_data\n\u001B[0;32m     22\u001B[0m )\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Start training\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1527\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[0;32m   1524\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[0;32m   1525\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[0;32m   1526\u001B[0m )\n\u001B[1;32m-> 1527\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1532\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1749\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1746\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_rng_state(resume_from_checkpoint)\n\u001B[0;32m   1748\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1749\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[0;32m   1750\u001B[0m \n\u001B[0;32m   1751\u001B[0m     \u001B[38;5;66;03m# Skip past any already trained steps if resuming training\u001B[39;00m\n\u001B[0;32m   1752\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m steps_trained_in_current_epoch \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1753\u001B[0m         steps_trained_in_current_epoch \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_utils.py:696\u001B[0m, in \u001B[0;36mRemoveColumnsCollator.__call__\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m    694\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: List[\u001B[38;5;28mdict\u001B[39m]):\n\u001B[0;32m    695\u001B[0m     features \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_remove_columns(feature) \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[1;32m--> 696\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\data\\data_collator.py:70\u001B[0m, in \u001B[0;36mdefault_data_collator\u001B[1;34m(features, return_tensors)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001B[39;00m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# have the same attributes.\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# on the whole batch.\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch_default_data_collator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m return_tensors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf_default_data_collator(features)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\data\\data_collator.py:119\u001B[0m, in \u001B[0;36mtorch_default_data_collator\u001B[1;34m(features)\u001B[0m\n\u001B[0;32m    117\u001B[0m     label \u001B[38;5;241m=\u001B[39m first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    118\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlong \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(label, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mfloat\n\u001B[1;32m--> 119\u001B[0m     batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m first \u001B[38;5;129;01mand\u001B[39;00m first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(first[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor):\n",
      "\u001B[1;31mValueError\u001B[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
